---
title: "Causal ML Homework 1 - code"
author: "Luis Armona and Jack Blundell"
date: "May 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

```{r load, echo=F, message=FALSE, warning=FALSE}
# set working directory
setwd("C:/Users/Jack/Documents/Git/Athey ML homework 1/AtheyMLhw1") # Jack
#setwd('/home/luis/AtheyMLhw1') #Luis

# clear things in RStudio
rm(list = ls())

# Call packages
library(ggplot2)
library(dplyr)
library(reshape2)
library(glmnet)
library(plotmo)
#library(pogs) # Issue with pogs (Jack)
library(balanceHD)

# set seed
#set.seed(12345)

# Load data
fname <- 'analysis/input/charitable_withdummyvariables.csv'
char <- read.csv(fname)
attach(char)

```

## 1. 

Estimate the average treatment effect (on total donation) and confidence interval, as well as probit for giving at all

```{r ate, echo=TRUE}

reg.ols <- lm(out_amountgive ~ treatment) 
summary(reg.ols) # significant at 90% but not 95% level
# Consistent with Table 4 of paper
confint(reg.ols, level=0.95) # CI

#probit regression
gave.probit <- glm(out_gavedum ~ treatment,family=binomial(link='probit'))
#convert coef to derivative
marginal.effect <- mean(dnorm(predict(gave.probit, type = "link")))*coef(gave.probit)
print(marginal.effect)
```


## 2. 

Drop some observations based on a selection rule
```{r drop, echo=TRUE}

# Generate restricted dataset, dropping all those missing key covariates
char.res <- char[ which(page18_39!=-999
                         & perbush!=-999
                         & median_hhincome!=-999), ] # drop all those with missings of key variables
prop.treat <- mean(char.res$treatment)
detach(char)
attach(char.res) # attach so don't have to call each time

#randomly censor individuals
#via a  complex, highly nonlinear fcn  of votes 4 bush in state,

ps.fcn <- function(v,c,pg,t){ # inputs: votes for bush, court cases, highest prev cont, treatment
  #v_t <- (v-.25)/.5
  v_t <- v
  #ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))/5
  #p<- (c*(acos(v_t))*atan(v_t^2)  - .5*exp(v_t))/4 + (t*((ihs_pg)) + (1-t))/2
  ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))
  p<- (1-t)*(c+1)*(acos(v_t)*atan(v_t) )/3 + 
      t*(.01+(-.01*ihs_pg^5 + 1*ihs_pg^3)/300)
  p<- pmin(pmax(0,p),1)
  return(p)
}

#story to accompany this fcn: ACLU wants to help those in trouble in "red states" but do not 
#feel they can make a difference in really, really red states so target donors less often

plot(seq(0,1,.001),ps.fcn(seq(0,1,.001),4,800,0)) #a plot of the function
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),3,800,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),2,200,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),1,200,0))

plot(char.res$hpa, ps.fcn(0,0,char.res$hpa,1))

# Selection rule
char.res$ps.select <- ps.fcn(char.res$perbush,char.res$cases,char.res$hpa,char.res$treatment) # hpa is highest previous contribution. cases is court cases from state which organization was involved.

# True propensity score via Bayes' theorem
char.res$ps.select.t <- ps.fcn(char.res$perbush,char.res$cases,char.res$hpa,1)
char.res$ps.select.c <- ps.fcn(char.res$perbush,char.res$cases,char.res$hpa,0)
char.res$ps.true <- (prop.treat*char.res$ps.select.t)/(prop.treat*char.res$ps.select.t + (1 - prop.treat)*char.res$ps.select.c)

# Plot CDF of this nonlinear function
ggplot(char.res,aes(x=ps.true))+ stat_ecdf()

# Set seed
set.seed(21) 

# Selection rule (=1 of uniform random [0,1] is lower, so those with higher ps.true more likely to be selected)
selection <- runif(nrow(char.res)) <= char.res$ps.select

char.censored <- char.res[selection,] #remove observations via propensity score rule

#ggplot(char.res,aes(x=perbush)) + geom_histogram()+xlim(c(0,1))

ggplot(char.censored,aes(x=ps.true)) + geom_histogram() +xlim(c(0,1))

#overlap in true propensity score
ggplot(char.censored,aes(x=ps.true,colour=factor(treatment))) + stat_ecdf()

ggplot(char.censored,aes(x=ps.true,y=hpa,colour=factor(treatment))) + geom_point()

```

Run regressions with and without dropped observations

```{r reg drop, echo=TRUE}

# New regression results with dropping

#Luis' PS generating rule
reg.censored <- lm(out_amountgive ~ treatment, data = char.censored) 
summary(reg.censored) 

# Old regression results (remember to drop missings to make comparable sample)
reg.ols.comp <- lm(out_amountgive ~ treatment, data = char.res) 
summary(reg.ols.comp) 
ate.true <- reg.ols.comp$coefficients[2] # label this 'true' ATE

```

Estimate bias function

```{r bias, echo=TRUE, message=FALSE, warning=FALSE}

#since we have some continuous covariates,
#use the Mahalanobis distance to get conditional means
#in a 'neighborhood' of each set of X's

bias.fcn <- function(ps,treat,y, covars){
  x<- covars
  #stdize covariates to be z-scores mean 0 sd 1
  for (j in 1:ncol(covars)){
    x[,j] = (x[,j] - mean(x[,j]) )/sd(x[,j])
  }
  covx<- cov(x)
  bias <- matrix(NaN,nrow=length(y),ncol=1)
  mu.t <- mean(y[treat])
  mu.c <- mean(y[!treat])
  p <- mean(treat)
  for (i in 1:length(y)){
    if (i%%1000==0){
      print(i)
    }
    maxdist<- .1 #the max Mahalanobis distance
    #to compute conditional means for bias function
    distances <- mahalanobis(x,center=x[i],cov<-covx)
    while (length(y[distances <= maxdist & treat]) == 0 | 
           length(y[distances <= maxdist & !treat]) == 0){
        maxdist <- maxdist+.1
    }
    mu.t.X = mean(y[distances <= maxdist & treat])
    mu.c.X = mean(y[distances <= maxdist & !treat])
    bias[i] <- (ps[i]-p)*( p*(mu.c.X-mu.c) + (1-p)*(mu.t.X-mu.t)  )
  }
  
  return(bias)
}
covars.ps <- cbind(char.censored$hpa,char.censored$cases,char.censored$perbush) #Xs relevant for p-score
char.censored$bias <- bias.fcn(char.censored$ps.true,char.censored$treatment,
                 char.censored$out_amountgive, covars.ps)
ggplot(char.censored,aes(x=bias)) +geom_histogram(fill=I("white"),col=I("black"))
E.bias = mean(char.censored$bias)/mean(char.censored$treatment)*(1-mean(char.censored$treatment))
print(E.bias)
summary(char.censored$bias)

```

## 3.

Traditional methods for estimating ATE
```{r trad, echo=TRUE}


# propensity score weighting ATE
#first, estimate the propensity score with a logit regression using all covars
#since in this exercise we should not know the "ground truth" propensity score
covars.all <- char.censored[,c(14:22,23:63)] #skip the state indicator used for summ stats
#formula to interact all covariates no interactions for missing dummies.
#for tractability, we interact individ. covars with each other, and state vars with each other
#create design matrix storing all features
covars.regular <-char.censored[,c(14:22,23:44)]
covars.missing <- char.censored[,c(45:63)]
int.level = 2 #the degree of interaction between covariates that are not missing dummies
covars.poly.str = paste('(', paste(names(covars.regular)[1:9],collapse='+'),')^',int.level,
                        ' + (', paste(names(covars.regular)[11:31],collapse='+'),')^',int.level,
                        ' + ',paste(names(covars.missing),collapse='+'),sep='') 
#covars.poly.str = paste('(', paste(names(covars.regular),collapse='+'),')^',int.level,
#                        ' + ',paste(names(covars.missing),collapse='+'),sep='') 
covars.poly <-model.matrix(as.formula(paste('~ ',covars.poly.str)),data=char.censored)

#the logit with all uninteracted covariates
#ps.formula <- paste('treatment ~ ',paste(names(covars.all),collapse='+'))
#m.ps <- glm(ps.formula,  family = binomial(), data = char.censored)

#OLS for p-score with interacted covariates 
#(NOTE: logit w/ interactions doesnt converge)
ps.formula <- paste('treatment ~ ',covars.poly.str)
m.ps <- lm(ps.formula, data = char.censored)
char.censored$ps.est <- pmax(pmin(predict(m.ps,type='response'),1),0)

summary(char.censored$ps.est)
#compare estimated p-score w/ real p-score
ggplot(melt(char.censored[,c('ps.true','ps.est')]),aes(x=value,colour=variable)) + geom_density(alpha=.2)

char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.true[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.true[char.censored$treatment == 0]))


#regular propensity score weighting
ate.ps <- mean(char.censored$out_amountgive*(char.censored$treatment 
               - char.censored$ps.est)/(char.censored$ps.est*(1 - char.censored$ps.est)), na.rm = T)
print(ate.ps)


# direct regression analysis ATE;
# control for Xs linearly
#ols.formula <- paste('out_amountgive ~ treatment +', paste(names(covars.all),collapse='+'),sep='')
#reg.ols <- lm(ols.formula, data=char.censored)

#w/ full interactions
ols.formula <- paste('out_amountgive ~ treatment +', covars.poly.str,sep='')
reg.ols <- lm(ols.formula, data=char.censored)
print(reg.ols$coefficients['treatment'])
#does much better; pretty close to true ATE

# traditional double robust analysis weighting using inverse propensity score weighting; 
# the lm command in R has a weights option.
pweight.reg <- lm(ols.formula, weights = w.ate, data = char.censored[is.finite(char.censored$w.ate),],na.action=na.exclude)
#summary(pweight.reg)
print(pweight.reg$coefficients['treatment'])

ATEs.classic <- cbind(ate.ps,
                          reg.ols$coefficients['treatment'],
                          pweight.reg$coefficients['treatment'])
colnames(ATEs.classic) <- c("PS Weighting",'OLS w/ Controls','Traditional DR OLS w/ IPS Weights')
#gives a similar answer to non-weighted reg


```

Re-estimate the above using lasso to estimate the propensity score

```{r lasso, echo=TRUE}

# Use full set of interactions in lasso
ps.m.cv <- cv.glmnet(covars.poly,char.censored$treatment, family='binomial')
#coef(ps.m.cv,s='lambda.min')
char.censored$ps.lasso <- predict(ps.m.cv,covars.poly,s='lambda.min',type='response')


#compare the method's generated p-scores
ggplot(melt(char.censored[,c('ps.true','ps.est','ps.lasso')]),aes(x=value,colour=variable)) + geom_density(alpha=.2)
#we get a worse p-score since it is regularized/shrunken so more concentrated

#redo above methods (weighted mean, DR weights)
char.censored$w.ate.lasso[char.censored$treatment == 1] <-  1/char.censored$ps.lasso[char.censored$treatment == 1]
char.censored$w.ate.lasso[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.lasso[char.censored$treatment == 0]))

#propensity score weighting
ate.ps.lasso <- mean(char.censored$out_amountgive*(char.censored$treatment 
               - char.censored$ps.lasso)/(char.censored$ps.lasso*(1 - char.censored$ps.lasso)), na.rm = T)
print(ate.ps.lasso)

#DR IPR weighting
pweight.lasso.reg <- lm(ols.formula, weights = w.ate.lasso, data = char.censored)
#summary(pweight.reg)
print(pweight.lasso.reg$coefficients['treatment'])
#DR method performs better now
ATEs.regPS <- cbind(ate.ps.lasso,pweight.lasso.reg$coefficients['treatment'])
colnames(ATEs.regPS) <- c('Regularized PS Weighted ATE','Classic Double Robust w/ Regularized PS')

```

Single-equation lasso of Y on X and W
```{r singlass, echo=TRUE}

#set penalty coef for treatment to zero
p.fac = rep(1, ncol(covars.poly)+1)
p.fac[1]=0
lasso.reg <- cv.glmnet(cbind(char.censored$treatment,covars.poly),char.censored$out_amountgive,penalty.factor = p.fac,intercept=FALSE)
lasso.coef<-coef(lasso.reg,s='lambda.min')
print(lasso.coef['',]) #the treatment coef
#closer to zero, but actually the non-lassoed OLS estimate performed better


```

Belloni-Chernozhukov-Hansen method

```{r bch, echo=TRUE}

#use CV to get union of two sets
#already have p-score regularized estimation
psreg.vars<-rownames(coef(ps.m.cv,s='lambda.min'))
psreg.vars<-psreg.vars[as.logical(coef(ps.m.cv,s='lambda.min')!=0)]
#reduced form outcome reg
yreg.rf.cv<-cv.glmnet(covars.poly,char.censored$out_amountgive)
yreg.vars<-rownames(coef(yreg.rf.cv,s='lambda.min'))
yreg.vars<-yreg.vars[as.logical(coef(yreg.rf.cv,s='lambda.min')!=0)]
ds.vars <- union(yreg.vars,psreg.vars)
ds.vars <- ds.vars[-1]#remove intercept
doubleselect.formula <- paste('out_amountgive ~ treatment + ',paste(ds.vars,collapse='+'),sep='')
reg.DoubleSelection <- lm(doubleselect.formula,data=char.censored)
#summary(reg.DoubleSelection)
print(reg.DoubleSelection$coefficients['treatment'])
#fairly close to true ATE; still only about as good as regular OLS

```

# Plot how ATE changes with regularization

```{r regularization, echo=TRUE}

# Lasso of outcome on treatment and covars


p.fac = rep(1, ncol(covars.poly)+1)
p.fac[1]=0
lasso.reg.pen <- glmnet(cbind(char.censored$treatment,covars.poly),char.censored$out_amountgive,penalty.factor = p.fac)

# Plot using default plot
#plot(lasso.reg.pen, label = T, xvar = "lambda")

# Plot using plotmo (more options)
plot_glmnet(lasso.reg.pen,
            xvar = c("lambda"),
            label = 10, nresponse = NA, grid.col = NA, s = NA)

# No option to plot path of single coefficient, so make plot ourselves

lambdas <- seq(from = 0, to = 1, by = 0.01) # Range of lambdas to try
t.coef <- coef(lasso.reg.pen,lambdas)[2,] # Pick out treatment coefficient
ggplot(data = NULL,aes(x=lambdas, y=t.coef)) +
  geom_line(colour="blue", size=1.5) +
  xlab("Lambda") + ylab("Treatment coefficient")

# As lambda increases, treatment effect increases as expected. Treatment effect ranges from that of full OLS 
# controlling for covariates to simple OLS with no controls.

```

## 4.

Double machine learning to estimate ATE

```{r double ml, echo=TRUE}

# LASSO of Y on X

lasso.YX <- cv.glmnet(covars.poly,char.censored$out_amountgive)
#coef(lasso.YX, s = "lambda.min")
lasso.YX.res <- predict(lasso.YX ,covars.poly,s=lasso.YX$lambda.min) - char.censored$out_amountgive # residuals

#summary(lasso.YX.res) # very skewed

# LASSO of W on X 

lasso.WX <- cv.glmnet(covars.poly,char.censored$treatment)
#coef(lasso.WX, s = "lambda.min")
lasso.WX.res <- predict(lasso.WX ,covars.poly,s=lasso.WX$lambda.min) - char.censored$treatment # residuals
#summary(lasso.WX.res) # very skewed

# Residual on residual regression

reg.res <- lm(lasso.YX.res ~ lasso.WX.res)
summary(reg.res)

```

Approximate residual balancing

```{r resid balance, echo=TRUE, eval= FALSE}

#tau.hat = residualBalance.ate(covars.poly, char.censored$out_amountgive,
                              char.censored$treatment, estimate.se = TRUE,  optimizer = "pogs")
#print(paste("true tau:", ate.true)) # 0.166066838677917
#print(paste("point estimate:", round(tau.hat[1], 4))) #1.2675
#print(paste0("95% CI for tau: (", round(tau.hat[1] - 1.96 * tau.hat[2], 2), ", ", round(tau.hat[1] + 1.96 * tau.hat[2], 2), ")"))

```

``` {r save LA, eval = FALSE}
ATEs.ML <- cbind(lasso.coef['',],
                 reg.DoubleSelection$coefficients['treatment'],
                 coef(reg.res)[2],
                 tau.hat[1])
colnames(ATEs.ML) <- c('Direct Lasso on Outcome',
                       'Double Selection','Lasso Residual-on-Residual','Approx. Residual Balancing')
#sink('estimates.txt')
```

``` {r save JB}
ATEs.ML <- cbind(lasso.coef['',],
                 reg.DoubleSelection$coefficients['treatment'],
                 coef(reg.res)[2])
colnames(ATEs.ML) <- c('Direct Lasso on Outcome',
                       'Double Selection','Lasso Residual-on-Residual')
#sink('estimates.txt')
```
Print results for comparison
```{r res}
print(paste('True ATE: ',ate.true))
print(t(ATEs.classic))
print(t(ATEs.regPS))
print(t(ATEs.ML))
#sink()
```