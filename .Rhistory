# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
library(dplyr)
library(reshape2)
library(glmnet)
library(plotmo)
library(pogs)
library(balanceHD)
# set seed
set.seed(12345)
############################################################
# Load data
fname <- 'analysis/input/charitable_withdummyvariables.csv'
char <- read.csv(fname)
attach(char) # attach so don't have to call each time
### Exploratory analysis
dim(char) # 50,083 obs, 63 vars
names(char)
head(char) # Look at first few entries of each var
# Treatment
summary(treatment) # Anyone who got any of the 27 treatments (3 match x 3 match size x 3 reccomended amount)
mean(treatment) # 67% treated
# Gives at all
summary(out_gavedum)
# Giving
summary(out_amountgive) # amount given. Highly skewed
hist(out_amountgive)
############################################################
### 1. Regression for average treatment effect
reg.ols <- lm(out_amountgive ~ treatment)
summary(reg.ols) # show results, significant at 90% but not 95% level
# Consistent with Table 4 of paper
confint(reg.ols, level=0.95) # CI
#probit regression
gave.probit <- glm(out_gavedum ~ treatment,family=binomial(link='probit'))
#convert coef to derivative
marginal.effect <- mean(dnorm(predict(gave_probit, type = "link")))*coef(gave_probit)
print(marginal.effect)
############################################################
### 2. Dropping some observations
# Try to find some drop of observations based on observables that changes the treatment effect
# look at some potential variables
summary(page18_39)
hist(page18_39[page18_39_missing!=1])
summary(perbush)
hist(perbush[perbush_missing!=1])
# Generate restricted dataset, dropping all those missing key covariates
char.res <- char[ which(page18_39!=-999
& perbush!=-999
& median_hhincome!=-999), ] # drop all those with missings of key variables
detach(char)
attach(char.res) # attach so don't have to call each time
#char_res$drop <- 0 # variable telling us to drop or not
# Make threshold rule for dropping (alternatively do with random variable)
#char_res$thres <- perbush #+ 0.1*(1+perbush)^2 - 0.1*page18_39*perbush #- page18_39 - page18_39^2 + perbush^2
#summary(char_res$thres)
#char_res$drop[char_res$thres <= 0.5] <- 1
#mean(char_res$drop) # drop 23 % of obs
#char_res_d <- char_res[which(char_res$drop == 0),]
##############################
#Alternative rule:
#randomly censor individuals
#via a  complex, highly nonlinear fcn  of votes 4 bush in state,
#
ps.fcn <- function(v,c,pg,t){
#v_t <- (v-.25)/.5
v_t <- v
#ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))/5
#p<- (c*(acos(v_t))*atan(v_t^2)  - .5*exp(v_t))/4 + (t*((ihs_pg)) + (1-t))/2
ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))
p<- (1-t)*(c+1)*(acos(v_t)*atan(v_t) )/3 +
t*(.01+(-.01*ihs_pg^5 + 1*ihs_pg^3)/300)
p<- pmin(pmax(0,p),1)
return(p)
}
#story to accompany this fcn: ACLU wants to help those in trouble in "red states" but do not
#feel they can make a difference in really, really red states so target donors less often
plot(seq(0,1,.001),ps.fcn(seq(0,1,.001),4,800,0)) #a plot of the function
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),3,800,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),2,200,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),1,200,0))
plot(char.res$hpa, ps.fcn(0,0,char.res$hpa,1))
#char$mibush=char$perbush==-999
#char$perbush[char$mibush]=.5
# Input from highly non-linear function
char.res$ps.true <- ps.fcn(char.res$perbush,char.res$cases,char.res$hpa,char.res$treatment) # hpa is highest previous contribution. cases is court cases from state which organization was involved.
# Plot CDF of this nonlinear function
ggplot(char.res,aes(x=ps.true))+ stat_ecdf()
# Set seed
set.seed(21)
# Selection rule (=1 of uniform random [0,1] is lower, so those with higher ps.true more likely to be selected)
selection <- runif(nrow(char.res)) <= char.res$ps.true
char.censored <- char.res[selection,] #remove observations via propensity score rule
ggplot(char.res,aes(x=perbush)) + geom_histogram()+xlim(c(0,1))
ggplot(char.censored,aes(x=ps.true)) + geom_histogram() +xlim(c(0,1))
#overlap in true propensity score
ggplot(char.censored,aes(x=ps.true,colour=factor(treatment))) + stat_ecdf()
ggplot(char.censored,aes(x=ps.true,y=hpa,colour=factor(treatment))) + geom_point()
#there is clear overlap, but clearly assymetries going on with hpa as well
#################################
# New regression results with dropping
#jack's threshold rule
#reg_ols_drop <- lm(out_amountgive ~ treatment, data = char_res_d)
#summary(reg_ols_drop)
#Luis' PS generating rule
reg.censored <- lm(out_amountgive ~ treatment, data = char.censored)
summary(reg.censored)
# Old regression results (remember to drop missings to make comparable sample)
reg.ols.comp <- lm(out_amountgive ~ treatment, data = char.res)
summary(reg.ols.comp)
ate.true <- reg.ols.comp$coefficients[2]
# Check overlap (propensity score)
# estimate propensity score via logit regression
ps.mod <- glm(treatment ~ page18_39 + perbush + pwhite + pblack + median_hhincome + red0 + hpa + ltmedmra + freq + years + year5 + dormant + female + couple + nonlit,
family = binomial(), data = char.censored)
summary(ps.mod)
# Put propensity scores into dataframe with actual treatment
ps.df <- data.frame(pr.score = predict(ps.mod),
treatment = ps.mod$model$treatment)
head(ps.df)
ps.df$pr.score <- pmin(ps.df$pr.score,1)
# Generate graph (see https://stanford.edu/~ejdemyr/r-tutorials-archive/tutorial8.html#propensity-score-estimation)
labs <- paste("Actual treatment:", c("Treated", "Control"))
ps.df %>%
mutate(treatment = ifelse(treatment == 1, labs[1], labs[2])) %>%
ggplot(aes(x = pr.score)) +
geom_histogram(color = "white") +
facet_wrap(~treatment) +
xlab("Probability of treatment") +
theme_bw()
#################################
#Bias function under luis' PS rule
#since we have some continuous covariates,
#use the Mahalanobis distance to get conditional means
#in a 'neighborhood' of each set of X's
bias.fcn <- function(ps,treat,y, covars){
x<- covars
#stdize covariates to be z-scores mean 0 sd 1
for (j in 1:ncol(covars)){
x[,j] = (x[,j] - mean(x[,j]) )/sd(x[,j])
}
covx<- cov(x)
bias <- matrix(NaN,nrow=length(y),ncol=1)
mu.t <- mean(y[treat])
mu.c <- mean(y[!treat])
p <- mean(treat)
for (i in 1:length(y)){
if (i%%1000==0){
print(i)
}
maxdist<- .1 #the max Mahalanobis distance
#to compute conditional means for bias function
distances <- mahalanobis(x,center=x[i],cov<-covx)
while (length(y[distances <= maxdist & treat]) == 0 |
length(y[distances <= maxdist & !treat]) == 0){
maxdist <- maxdist+.1
}
mu.t.X = mean(y[distances <= maxdist & treat])
mu.c.X = mean(y[distances <= maxdist & !treat])
bias[i] <- (ps[i]-p)*( p*(mu.c.X-mu.c) + (1-p)*(mu.t.X-mu.t)  )
}
return(bias)
}
covars.ps <- cbind(char.censored$hpa,char.censored$cases,char.censored$perbush) #Xs relevant for p-score
char.censored$bias <- bias.fcn(char.censored$ps.true,char.censored$treatment,
char.censored$out_amountgive, covars.ps)
# propensity score weighting ATE
#first, estimate the propensity score with a logit regression using all covars
#since in this exercise we should not know the "ground truth" propensity score
covars.all <- char.censored[,c(14:22,23:63)] #skip the state indicator used for summ stats
#formula to interact all covariates no interactions for missing dummies.
#for tractability, we interact individ. covars with each other, and state vars with each other
#create design matrix storing all features
covars.regular <-char.censored[,c(14:22,23:44)]
covars.missing <- char.censored[,c(45:63)]
int.level = 2 #the degree of interaction between covariates that are not missing dummies
covars.poly.str = paste('(', paste(names(covars.regular)[1:9],collapse='+'),')^',int.level,
' + (', paste(names(covars.regular)[11:31],collapse='+'),')^',int.level,
' + ',paste(names(covars.missing),collapse='+'),sep='')
#covars.poly.str = paste('(', paste(names(covars.regular),collapse='+'),')^',int.level,
#                        ' + ',paste(names(covars.missing),collapse='+'),sep='')
covars.poly <-model.matrix(as.formula(paste('~ ',covars.poly.str)),data=char.censored)
#the logit with all uninteracted covariates
#ps.formula <- paste('treatment ~ ',paste(names(covars.all),collapse='+'))
#m.ps <- glm(ps.formula,  family = binomial(), data = char.censored)
#OLS for p-score with interacted covariates
#(NOTE: logit w/ interactions doesnt converge)
ps.formula <- paste('treatment ~ ',covars.poly.str)
m.ps <- lm(ps.formula, data = char.censored)
char.censored$ps.est <- pmax(pmin(predict(m.ps,type='response'),1),0)
summary(char.censored$ps.est)
#compare estimated p-score w/ real p-score
ggplot(melt(char.censored[,c('ps.true','ps.est')]),aes(x=value,colour=variable)) + geom_density(alpha=.2)
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.true[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.true[har.censored$treatment == 0]))
#regular propensity score weighting
ate.ps <- mean(char.censored$out_amountgive[char.censored$treatment==1]*
char.censored$w.ate[char.censored$treatment == 1],na.rm=TRUE) -
mean(char.censored$out_amountgive[char.censored$treatment==0]*
char.censored$w.ate[char.censored$treatment == 0],na.rm=TRUE)
print(ate.ps)
#gives a negative score!
#the logit with all uninteracted covariates
#ps.formula <- paste('treatment ~ ',paste(names(covars.all),collapse='+'))
#m.ps <- glm(ps.formula,  family = binomial(), data = char.censored)
#OLS for p-score with interacted covariates
#(NOTE: logit w/ interactions doesnt converge)
ps.formula <- paste('treatment ~ ',covars.poly.str)
m.ps <- lm(ps.formula, data = char.censored)
char.censored$ps.est <- pmax(pmin(predict(m.ps,type='response'),1),0)
summary(char.censored$ps.est)
#compare estimated p-score w/ real p-score
ggplot(melt(char.censored[,c('ps.true','ps.est')]),aes(x=value,colour=variable)) + geom_density(alpha=.2)
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.true[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.true[char.censored$treatment == 0]))
#regular propensity score weighting
ate.ps <- mean(char.censored$out_amountgive[char.censored$treatment==1]*
char.censored$w.ate[char.censored$treatment == 1],na.rm=TRUE) -
mean(char.censored$out_amountgive[char.censored$treatment==0]*
char.censored$w.ate[char.censored$treatment == 0],na.rm=TRUE)
print(ate.ps)
#gives a negative score!
summary(char.censored$ps.true)
#Luis' PS generating rule
reg.censored <- lm(out_amountgive ~ treatment, data = char.censored)
summary(reg.censored)
# Old regression results (remember to drop missings to make comparable sample)
reg.ols.comp <- lm(out_amountgive ~ treatment, data = char.res)
summary(reg.ols.comp)
ate.true <- reg.ols.comp$coefficients[2]
summary(char.res$ps.true)
summary(char.censored$ps.true)
summary(char.censored$ps.true)
mean(char.censored$out_amountgive[char.censored$treatment==1]*
char.censored$w.ate[char.censored$treatment == 1],na.rm=TRUE)
mean(char.censored$out_amountgive)
mean(char.censored$out_amountgive[char.censored$treatment==0]*
char.censored$w.ate[char.censored$treatment == 0],na.rm=TRUE)
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.est[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.est[char.censored$treatment == 0]))
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.est[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 1] <- char.censored$w.ate[char.censored$treatment == 1] + 1 - mean(char.censored$w.ate[char.censored$treatment == 1])
char.censored$w.ate[char.censored$treatment == 0] <- char.censored$w.ate[char.censored$treatment == 0] + 1 - mean(char.censored$w.ate[char.censored$treatment == 0])
#char$perbush[char$mibush]=.5
# Input from highly non-linear function
char.res$ps.true <- ps.fcn(char.res$perbush,char.res$cases,char.res$hpa,char.res$treatment) # hpa is highest previous contribution. cases is court cases from state which organization was involved.
# Plot CDF of this nonlinear function
ggplot(char.res,aes(x=ps.true))+ stat_ecdf()
# Set seed
set.seed(21)
# Selection rule (=1 of uniform random [0,1] is lower, so those with higher ps.true more likely to be selected)
selection <- runif(nrow(char.res)) <= char.res$ps.true
char.censored <- char.res[selection,] #remove observations via propensity score rule
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.est[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.est[char.censored$treatment == 0]))
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.est[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 1] <- char.censored$w.ate[char.censored$treatment == 1] + 1 - mean(char.censored$w.ate[char.censored$treatment == 1])
summary(char.censored$w.ate)
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.est[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.est[char.censored$treatment == 0]))
summary(char.censored$w.ate)
char.censored$ps.est <- pmax(pmin(predict(m.ps,type='response'),1),0)
summary(char.censored$ps.est)
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.true[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.true[char.censored$treatment == 0]))
summary(char.censored$w.ate)
char.censored$w.ate[char.censored$treatment == 1] <- char.censored$w.ate[char.censored$treatment == 1] + 1 - mean(char.censored$w.ate[char.censored$treatment == 1])
char.censored$w.ate[char.censored$treatment == 0] <- char.censored$w.ate[char.censored$treatment == 0] + 1 - mean(char.censored$w.ate[char.censored$treatment == 0])
summary(char.censored$w.ate)
#regular propensity score weighting
ate.ps <- mean(char.censored$out_amountgive[char.censored$treatment==1]*
char.censored$w.ate[char.censored$treatment == 1],na.rm=TRUE) -
mean(char.censored$out_amountgive[char.censored$treatment==0]*
char.censored$w.ate[char.censored$treatment == 0],na.rm=TRUE)
print(ate.ps)
ate.ps <- mean(char.censored$out_amountgive*(char.censored$treatment - char.censored$ps.true)/(char.censored$ps.true*(1 - char.censored$ps.true)))
print(ate.ps)
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.true[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.true[char.censored$treatment == 0]))
#char.censored$w.ate[char.censored$treatment == 1] <- char.censored$w.ate[char.censored$treatment == 1] + 1 - mean(char.censored$w.ate[char.censored$treatment == 1])
#char.censored$w.ate[char.censored$treatment == 0] <- char.censored$w.ate[char.censored$treatment == 0] + 1 - mean(char.censored$w.ate[char.censored$treatment == 0])
#regular propensity score weighting
ate.ps <- mean(char.censored$out_amountgive*(char.censored$treatment - char.censored$ps.true)/(char.censored$ps.true*(1 - char.censored$ps.true)))
print(ate.ps)
ps.formula <- paste('treatment ~ ',covars.poly.str)
m.ps <- lm(ps.formula, data = char.censored)
char.censored$ps.est <- pmax(pmin(predict(m.ps,type='response'),1),0)
# propensity score weighting ATE
#first, estimate the propensity score with a logit regression using all covars
#since in this exercise we should not know the "ground truth" propensity score
covars.all <- char.censored[,c(14:22,23:63)] #skip the state indicator used for summ stats
#formula to interact all covariates no interactions for missing dummies.
#for tractability, we interact individ. covars with each other, and state vars with each other
#create design matrix storing all features
covars.regular <-char.censored[,c(14:22,23:44)]
covars.missing <- char.censored[,c(45:63)]
int.level = 2 #the degree of interaction between covariates that are not missing dummies
covars.poly.str = paste('(', paste(names(covars.regular)[1:9],collapse='+'),')^',int.level,
' + (', paste(names(covars.regular)[11:31],collapse='+'),')^',int.level,
' + ',paste(names(covars.missing),collapse='+'),sep='')
#covars.poly.str = paste('(', paste(names(covars.regular),collapse='+'),')^',int.level,
#                        ' + ',paste(names(covars.missing),collapse='+'),sep='')
covars.poly <-model.matrix(as.formula(paste('~ ',covars.poly.str)),data=char.censored)
#the logit with all uninteracted covariates
#ps.formula <- paste('treatment ~ ',paste(names(covars.all),collapse='+'))
#m.ps <- glm(ps.formula,  family = binomial(), data = char.censored)
#OLS for p-score with interacted covariates
#(NOTE: logit w/ interactions doesnt converge)
ps.formula <- paste('treatment ~ ',covars.poly.str)
m.ps <- lm(ps.formula, data = char.censored)
char.censored$ps.est <- pmax(pmin(predict(m.ps,type='response'),1),0)
summary(char.censored$ps.est)
#compare estimated p-score w/ real p-score
ggplot(melt(char.censored[,c('ps.true','ps.est')]),aes(x=value,colour=variable)) + geom_density(alpha=.2)
char.censored$w.ate[char.censored$treatment == 1] <-  1/char.censored$ps.true[char.censored$treatment == 1]
char.censored$w.ate[char.censored$treatment == 0] <-  ( 1 / (1 - char.censored$ps.true[char.censored$treatment == 0]))
ate.ps <- mean(char.censored$out_amountgive*(char.censored$treatment - char.censored$ps.est)/(char.censored$ps.est*(1 - char.censored$ps.est)))
print(ate.ps)
#(NOTE: logit w/ interactions doesnt converge)
ps.formula <- paste('treatment ~ ',covars.poly.str)
m.ps <- lm(ps.formula, data = char.censored)
char.censored$ps.est <- pmax(pmin(predict(m.ps,type='response'),1),0)
summary(char.censored$ps.est)
#regular propensity score weighting
ate.ps <- mean(char.censored$out_amountgive*(char.censored$treatment - char.censored$ps.est)/(char.censored$ps.est*(1 - char.censored$ps.est)))
print(ate.ps)
ate.ps <- mean(char.censored$out_amountgive*(char.censored$treatment - char.censored$ps.est)/(char.censored$ps.est*(1 - char.censored$ps.est)), na.rm = T)
print(ate.ps)
ate.true
# Section numbers correspond to assignment page
############################################################
# set your working directory
setwd("C:/Users/Jack/Documents/Git/Athey ML homework 1/AtheyMLhw1") # Jack
#setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
library(dplyr)
library(reshape2)
library(glmnet)
library(plotmo)
library(pogs)
library(balanceHD)
# set seed
set.seed(12345)
############################################################
# Load data
fname <- 'analysis/input/charitable_withdummyvariables.csv'
char <- read.csv(fname)
attach(char) # attach so don't have to call each time
### Exploratory analysis
dim(char) # 50,083 obs, 63 vars
names(char)
head(char) # Look at first few entries of each var
# Treatment
summary(treatment) # Anyone who got any of the 27 treatments (3 match x 3 match size x 3 reccomended amount)
mean(treatment) # 67% treated
# Gives at all
summary(out_gavedum)
# Giving
summary(out_amountgive) # amount given. Highly skewed
hist(out_amountgive)
############################################################
### 1. Regression for average treatment effect
reg.ols <- lm(out_amountgive ~ treatment)
summary(reg.ols) # show results, significant at 90% but not 95% level
# Consistent with Table 4 of paper
confint(reg.ols, level=0.95) # CI
#probit regression
gave.probit <- glm(out_gavedum ~ treatment,family=binomial(link='probit'))
#convert coef to derivative
marginal.effect <- mean(dnorm(predict(gave.probit, type = "link")))*coef(gave.probit)
print(marginal.effect)
############################################################
### 2. Dropping some observations
# Try to find some drop of observations based on observables that changes the treatment effect
# look at some potential variables
summary(page18_39)
hist(page18_39[page18_39_missing!=1])
summary(perbush)
hist(perbush[perbush_missing!=1])
# Generate restricted dataset, dropping all those missing key covariates
char.res <- char[ which(page18_39!=-999
& perbush!=-999
& median_hhincome!=-999), ] # drop all those with missings of key variables
detach(char)
attach(char.res) # attach so don't have to call each time
#### Hard dropping (JB)
char.res$redred <- 0
char.res$redred[char.res$red0 == 1 & char.res$redcty == 1] <- 1
char.res$mbush <- 0
char.res$mbush[char.res$perbush >=.50 & char.res$perbush <=.525] <- 1
char.res$pdrop <- 0.3*(1 - (1/3)*(char.res$redred + char.res$perbush + char.res$redred*char.res$perbush))*char.res$treatment
hist(char.res$pdrop) # Probability of being dropped from the sample
selection <- runif(nrow(char.res)) >= char.res$pdrop
char.censored <- char.res[selection,] #remove observations via propensity score rule
##### estimate propensity score
ps.mod <- glm(treatment ~ perbush + red0 + redcty,
family = binomial(), data = char.censored)
summary(ps.mod)
# We see much more likely to be treated in red states with higher perbush
# Put propensity scores into dataframe with actual treatment
char.censored$ps.est <- predict(ps.mod,type='response')
# Check no estimated propensity scores of zero or one
hist(char.censored$ps.est)
bias.fcn <- function(ps,treat,y, covars){
x<- covars
#stdize covariates to be z-scores mean 0 sd 1
for (j in 1:ncol(covars)){
x[,j] = (x[,j] - mean(x[,j]) )/sd(x[,j])
}
covx<- cov(x)
bias <- matrix(NaN,nrow=length(y),ncol=1)
mu.t <- mean(y[treat])
mu.c <- mean(y[!treat])
p <- mean(treat)
for (i in 1:length(y)){
if (i%%1000==0){
print(i)
}
maxdist<- .1 #the max Mahalanobis distance
#to compute conditional means for bias function
distances <- mahalanobis(x,center=x[i],cov<-covx)
while (length(y[distances <= maxdist & treat]) == 0 |
length(y[distances <= maxdist & !treat]) == 0){
maxdist <- maxdist+.1
}
mu.t.X = mean(y[distances <= maxdist & treat])
mu.c.X = mean(y[distances <= maxdist & !treat])
bias[i] <- (ps[i]-p)*( p*(mu.c.X-mu.c) + (1-p)*(mu.t.X-mu.t)  )
}
return(bias)
}
covars.ps <- cbind(perbush, red0, redcty) #Xs relevant for p-score
char.censored$bias <- bias.fcn(char.censored$ps.est,char.censored$treatment,
char.censored$out_amountgive, covars.ps)
setwd("C:/Users/Jack/Documents/Git/Athey ML homework 1/AtheyMLhw1") # Jack
#setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
library(dplyr)
library(reshape2)
library(glmnet)
library(plotmo)
library(pogs)
library(balanceHD)
# set seed
set.seed(12345)
############################################################
# Load data
fname <- 'analysis/input/charitable_withdummyvariables.csv'
char <- read.csv(fname)
attach(char) # attach so don't have to call each time
### Exploratory analysis
dim(char) # 50,083 obs, 63 vars
names(char)
head(char) # Look at first few entries of each var
# Treatment
summary(treatment) # Anyone who got any of the 27 treatments (3 match x 3 match size x 3 reccomended amount)
mean(treatment) # 67% treated
# Gives at all
summary(out_gavedum)
# Giving
summary(out_amountgive) # amount given. Highly skewed
hist(out_amountgive)
############################################################
### 1. Regression for average treatment effect
reg.ols <- lm(out_amountgive ~ treatment)
summary(reg.ols) # show results, significant at 90% but not 95% level
# Consistent with Table 4 of paper
confint(reg.ols, level=0.95) # CI
#probit regression
gave.probit <- glm(out_gavedum ~ treatment,family=binomial(link='probit'))
#convert coef to derivative
marginal.effect <- mean(dnorm(predict(gave.probit, type = "link")))*coef(gave.probit)
print(marginal.effect)
############################################################
### 2. Dropping some observations
# Try to find some drop of observations based on observables that changes the treatment effect
# look at some potential variables
summary(page18_39)
hist(page18_39[page18_39_missing!=1])
summary(perbush)
hist(perbush[perbush_missing!=1])
# Generate restricted dataset, dropping all those missing key covariates
char.res <- char[ which(page18_39!=-999
& perbush!=-999
& median_hhincome!=-999), ] # drop all those with missings of key variables
detach(char)
attach(char.res) # attach so don't have to call each time
#### Hard dropping (JB)
char.res$redred <- 0
char.res$redred[char.res$red0 == 1 & char.res$redcty == 1] <- 1
char.res$mbush <- 0
char.res$mbush[char.res$perbush >=.50 & char.res$perbush <=.525] <- 1
char.res$pdrop <- 0.3*(1 - (1/3)*(char.res$redred + char.res$perbush + char.res$redred*char.res$perbush))*char.res$treatment
hist(char.res$pdrop) # Probability of being dropped from the sample
selection <- runif(nrow(char.res)) >= char.res$pdrop
char.censored <- char.res[selection,] #remove observations via propensity score rule
##### estimate propensity score
ps.mod <- glm(treatment ~ perbush + red0 + redcty,
family = binomial(), data = char.censored)
summary(ps.mod)
# Put propensity scores into dataframe with actual treatment
char.censored$ps.est <- predict(ps.mod,type='response')
# Check no estimated propensity scores of zero or one
hist(char.censored$ps.est)
