tau_s[x+1] = 0
V_s[x+1] = 0
}
}
#calc pop. ATE + variance
#assuming distro unknown
mean_null=0
V_tau = 0
tau = 0
for(x in 0:9){
tau = tau + tau_s[x+1]*(sum(X==x)/N)
V_tau = V_tau + V_s[x+1]*(sum(X==x)/N)^2
}
#do the t-stat calculation and assess rejection rate
t= (tau - mean_null)/sqrt(V_tau)
pval_tau = 1-pnorm(abs(t))
rejections[j] = (pval_tau <= size/2)
}
print('Pr(Reject Under Null of 0 ATE) Under Stratification:' )
print(mean(rejections))
power.stratification = mean(rejections)
################################
#Stratify on X
rejections=vector()
for (j in 1:nsims){
if(j%%1000==0){
print(j)
}
#gen data
X <- truncPoisson(N,lambda)
Y <- rnorm(N,beta*X,1)
#do treatment
W = (runif(N)<=prob_treat)
Y[W] = Y[W]+ATE
#reconfigure sample size for adjust for observations we aren't going to use
#do to inability to calculate statistics
N_adj=N
for(x in 0:9){
if ((sum(X==x & W==1) <2) | (sum(X==x & W==0) < 2) ) {
N_adj = N_adj + sum(X==x)
}
}
#calc within-strata ATE + variance
tau_s = vector(length=10)
V_s = vector(length=10)
for(x in 0:9){
if ((sum(X==x & W==1) >= 2) & (sum(X==x & W==0) >= 2) ){ #the regular case
tau_s[x+1] = mean(Y[W & X==x]) - mean(Y[!W & X==x]) #ATE est.
V_s[x+1] = sum( (Y[W & X==x]-mean(Y[W & X==x]))^2 )/(sum(W[X==x])-1)/sum(W[X==x]) +
sum( (Y[!W & X==x]-mean(Y[!W & X==x]))^2 )/(N_adj-sum(W[X==x])-1)/(N_adj-sum(W[X==x]))  #"conservative" estimate of variance
} else if ((sum(X==x & W==1) < 2) | (sum(X==x & W==0) < 2)) {
#deal with fringe case of not enough obs in stratum to calc variance
tau_s[x+1] = 0
V_s[x+1] = 0
}
}
#calc pop. ATE + variance
#assuming distro unknown
mean_null=0
V_tau = 0
tau = 0
for(x in 0:9){
tau = tau + tau_s[x+1]*(sum(X==x)/N_adj)
V_tau = V_tau + V_s[x+1]*(sum(X==x)/N_adj)^2
}
#do the t-stat calculation and assess rejection rate
t= (tau - mean_null)/sqrt(V_tau)
pval_tau = 1-pnorm(abs(t))
rejections[j] = (pval_tau <= size/2)
}
print('Pr(Reject Under Null of 0 ATE) Under Stratification:' )
print(mean(rejections))
power.stratification = mean(rejections)
#pset2.r
#do some power calculations
#varying the design of the experiment
#random, stratification, clustering, etc.
####
#truncated poisson distribution
#from 0 to 9
truncPoisson <- function(n,lambda){
ub = 9 #the upper bound of the truncated poisson distro
x = rpois(n,lambda)
oob=sum(x>ub) #num out of bound values generated
while (oob>0){
x[x>ub] <- rpois(oob,lambda)
oob=sum(x>ub)
#print(oob)
}
return(x)
}
#######
#randomly assign fraction p of n obs to treatment
assignTreat <- function(n,p){
uni<- runif(n)
W<- uni>=quantile(uni,p)
return(W)
}
#some of the primitives of the problem
N=1000 #population size
lambda=5 #mean of covariate
V_y = 1 #variance of outcome
beta=1 #for the multiplier on outcome mean
size=0.05 #so reject if pval <size
ATE=0.05 # the actual treatment effect
prob_treat = .5 #probability of assignment to treatment
nsims = 10000 #number of sims per power calc
set.seed(0219)
#prob(reject null | true tau)
#since data is normal, let's do a mean test (variance known so no need to estimate it)
################################
#Complete Randomization
#coin flips
rejections=vector()
for (j in 1:nsims){
if(j%%1000==0){
print(j)
}
#gen data
X <- truncPoisson(N,lambda)
Y <- rnorm(N,beta*X,1)
#do treatment
W = (runif(N)<=prob_treat)
Y[W] = Y[W]+ATE
#calc ATE
tau = mean(Y[W]) - mean(Y[!W]) #ATE est.
#if underlying distro is unknown
mean_null= 0
V_tau = sum( (Y[W]-mean(Y[W]))^2 )/(sum(W)-1)/sum(W) +
sum( (Y[!W]-mean(Y[!W]))^2 )/(N-sum(W)-1)/(N-sum(W))  #"conservative" estimate
#do the t-stat calculation and assess rejection rate
t= (tau - mean_null)/sqrt(V_tau)
pval_tau = 1-pnorm(abs(t))
rejections[j] = (pval_tau <= size/2)
}
print('Pr(Reject Under Null of 0 ATE) Under Complete Randomization:' )
print(mean(rejections))
power.completeRandomization = mean(rejections)
################################
#Stratify on X
rejections=vector()
for (j in 1:nsims){
if(j%%1000==0){
print(j)
}
#gen data
X <- truncPoisson(N,lambda)
Y <- rnorm(N,beta*X,1)
#do treatment
W = (runif(N)<=prob_treat)
Y[W] = Y[W]+ATE
#reconfigure sample size for adjust for observations we aren't going to use
#do to inability to calculate statistics
N_adj=N
for(x in 0:9){
if ((sum(X==x & W==1) <2) | (sum(X==x & W==0) < 2) ) {
N_adj = N_adj + sum(X==x)
}
}
#calc within-strata ATE + variance
tau_s = vector(length=10)
V_s = vector(length=10)
for(x in 0:9){
if ((sum(X==x & W==1) >= 2) & (sum(X==x & W==0) >= 2) ){ #the regular case
tau_s[x+1] = mean(Y[W & X==x]) - mean(Y[!W & X==x]) #ATE est.
V_s[x+1] = sum( (Y[W & X==x]-mean(Y[W & X==x]))^2 )/(sum(W[X==x])-1)/sum(W[X==x]) +
sum( (Y[!W & X==x]-mean(Y[!W & X==x]))^2 )/(N_adj-sum(W[X==x])-1)/(N_adj-sum(W[X==x]))  #"conservative" estimate of variance
} else if ((sum(X==x & W==1) < 2) | (sum(X==x & W==0) < 2)) {
#deal with fringe case of not enough obs in stratum to calc variance
tau_s[x+1] = 0
V_s[x+1] = 0
}
}
#calc pop. ATE + variance
#assuming distro unknown
mean_null=0
V_tau = 0
tau = 0
for(x in 0:9){
tau = tau + tau_s[x+1]*(sum(X==x)/N_adj)
V_tau = V_tau + V_s[x+1]*(sum(X==x)/N_adj)^2
}
#do the t-stat calculation and assess rejection rate
t= (tau - mean_null)/sqrt(V_tau)
pval_tau = 1-pnorm(abs(t))
rejections[j] = (pval_tau <= size/2)
}
print('Pr(Reject Under Null of 0 ATE) Under Stratification:' )
print(mean(rejections))
power.stratification = mean(rejections)
#################################
#Clustering
#randomize clusters of X_i to be treated
#then calculate the ATE
#calculate average cluster treatment effect
#using fact that TE is constant through clusters
rejections=vector()
for (j in 1:nsims){
if(j%%1000==0){
print(j)
}
#gen data
X <- truncPoisson(N,lambda)
Y <- rnorm(N,beta*X,1)
#do treatment; cluster by X values
W=vector(length=N)
treatClusts <- (runif(10)<=prob_treat)
#rule out situations where all clusters are assigned to treatment or control
while (sum(treatClusts)==0 | sum(treatClusts)==10){
treatClusts <- (runif(10)<=prob_treat)
}
clustCount <- vector(length=10) #keep track of cluster sizes
for(x in 0:9){
W[X==x] <- treatClusts[x+1]
clustCount[x+1] <- sum(X==x)
}
Y[W] = Y[W]+ATE
#calc ATE
Y_g = vector(length=10)
Y_t=0
Y_c=0
for(x in 0:9){
if (clustCount[x+1]==0) next #skip clusters without any observations
Y_g[x+1] = mean(Y[X==x])
Y_t = Y_t + Y_g[x+1]*sum(X==x)*(treatClusts[x+1]==1)
Y_c = Y_c + Y_g[x+1]*sum(X==x)*(treatClusts[x+1]==0)
}
Y_t = Y_t/sum(W)
Y_c = Y_c/(N-sum(W))
#tau = sum(Y_g[treatClusts])/sum(treatClusts) - sum(Y_g[!treatClusts])/(10-sum(treatClusts))
#following ch.7 t-test calcs of reading linked by guido
tau = Y_t - Y_c
#calc intracluster correlation coefficient
MSC=0
MSW=0
for(x in 0:9){
if (clustCount[x+1]==0) next
if (treatClusts[x+1]==1){
MSC = MSC + sum(X==x)*(Y_g[x+1]-Y_t)^2/(10-2)
} else{
MSC = MSC + sum(X==x)*(Y_g[x+1]-Y_c)^2/(10-2)
}
MSW = MSW + sum((Y[X==x]-Y_g[x+1])^2)/(N-10)
}
m0 = (N - sum(clustCount[treatClusts]^2)/sum(W) -
sum(clustCount[!treatClusts]^2)/(N-sum(W)) )/(10-2)
IC_corr = (MSC - MSW)/(MSC + (m0-1)*MSW)
#now calc adjusted SEs (again from ch.7 cluster reading)
C_t =  1 + (sum(clustCount[treatClusts]^2)/sum(W) - 1)*IC_corr
C_c =  1 + (sum(clustCount[!treatClusts]^2)/(N-sum(W)) - 1)*IC_corr
V_tau = sum((Y-mean(Y))^2)/(N-1) * (C_t/sum(W) + C_c/(N-sum(W)))
#assess t-stat
mean_null = 0
t  = (tau - mean_null)/sqrt(V_tau)
pval_tau = 1-pt(abs(t),10-2) #use actual t distro since 8 df is decently low approx to normal
rejections[j] = (pval_tau <= size/2)
}
print('Pr(Reject Under Null of 0 ATE) Under Clustered Randomization:' )
print(mean(rejections))
power.clusters = mean(rejections)
library(R.matlab)
library(network)
library(Matrix)
install.packages('R.matlab')
install.packages('network')
library(R.matlab)
library(network)
library(Matrix)
# Luis Armona and Jack Blundell
# Stanford University
# Spring 2017
# Section numbers correspond to assignment page
############################################################
# set your working directory
#setwd("C:/Users/Jack/Documents/Git/Athey ML homework 1/AtheyMLhw1") # Jack
setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
library(dplyr)
library(reshape2)
library(glmnet)
library(plotmo)
library(pogs)
library(balanceHD)
# set seed
set.seed(12345)
############################################################
# Load data
fname <- 'analysis/input/charitable_withdummyvariables.csv'
char <- read.csv(fname)
attach(char) # attach so don't have to call each time
### Exploratory analysis
dim(char) # 50,083 obs, 63 vars
names(char)
head(char) # Look at first few entries of each var
# Treatment
summary(treatment) # Anyone who got any of the 27 treatments (3 match x 3 match size x 3 reccomended amount)
mean(treatment) # 67% treated
# Gives at all
summary(out_gavedum)
# Giving
summary(out_amountgive) # amount given. Highly skewed
hist(out_amountgive)
############################################################
### 1. Regression for average treatment effect
reg.ols <- lm(out_amountgive ~ treatment)
summary(reg.ols) # show results, significant at 90% but not 95% level
# Consistent with Table 4 of paper
confint(reg.ols, level=0.95) # CI
#probit regression
gave.probit <- glm(out_gavedum ~ treatment,family=binomial(link='probit'))
#convert coef to derivative
marginal.effect <- mean(dnorm(predict(gave_probit, type = "link")))*coef(gave_probit)
print(marginal.effect)
############################################################
### 2. Dropping some observations
# Try to find some drop of observations based on observables that changes the treatment effect
# look at some potential variables
summary(page18_39)
hist(page18_39[page18_39_missing!=1])
summary(perbush)
hist(perbush[perbush_missing!=1])
# Generate restricted dataset, dropping all those missing key covariates
char.res <- char[ which(page18_39!=-999
& perbush!=-999
& median_hhincome!=-999), ] # drop all those with missings of key variables
detach(char)
attach(char.res) # attach so don't have to call each time
#char_res$drop <- 0 # variable telling us to drop or not
# Make threshold rule for dropping (alternatively do with random variable)
#char_res$thres <- perbush #+ 0.1*(1+perbush)^2 - 0.1*page18_39*perbush #- page18_39 - page18_39^2 + perbush^2
#summary(char_res$thres)
#char_res$drop[char_res$thres <= 0.5] <- 1
#mean(char_res$drop) # drop 23 % of obs
#char_res_d <- char_res[which(char_res$drop == 0),]
##############################
#Alternative rule:
#randomly censor individuals
#via a  complex, highly nonlinear fcn  of votes 4 bush in state,
#
ps.fcn <- function(v,c,pg,t){
#v_t <- (v-.25)/.5
v_t <- v
#ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))/5
#p<- (c*(acos(v_t))*atan(v_t^2)  - .5*exp(v_t))/4 + (t*((ihs_pg)) + (1-t))/2
ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))
p<- (1-t)*(c+1)*(acos(v_t)*atan(v_t) )/3 +
t*(.01+(-.01*ihs_pg^5 + 1*ihs_pg^3)/200)
p<- pmin(pmax(0,p),1)
return(p)
}
#story to accompany this fcn: ACLU wants to help those in trouble in "red states" but do not
#feel they can make a difference in really, really red states so target donors less often
plot(seq(0,1,.001),ps.fcn(seq(0,1,.001),4,800,0)) #a plot of the function
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),3,800,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),2,200,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),1,200,0))
plot(char.res$hpa, ps.fcn(0,0,char.res$hpa,1))
#char$mibush=char$perbush==-999
#char$perbush[char$mibush]=.5
# Input from highly non-linear function
char.res$ps.true <- ps.fcn(char.res$perbush,char.res$cases,char.res$hpa,char.res$treatment) # hpa is highest previous contribution. cases is court cases from state which organization was involved.
# Plot CDF of this nonlinear function
ggplot(char.res,aes(x=ps.true))+ stat_ecdf()
# Set seed
set.seed(21)
# Selection rule (=1 of uniform random [0,1] is lower, so those with higher ps.true more likely to be selected)
selection <- runif(nrow(char.res)) <= char.res$ps.true
char.censored <- char.res[selection,] #remove observations via propensity score rule
ggplot(char.res,aes(x=perbush)) + geom_histogram()+xlim(c(0,1))
ggplot(char.censored,aes(x=ps.true)) + geom_histogram() +xlim(c(0,1))
#overlap in true propensity score
ggplot(char.censored,aes(x=ps.true,colour=factor(treatment))) + stat_ecdf()
ggplot(char.censored,aes(x=ps.true,y=hpa,colour=factor(treatment))) + geom_point()
#there is clear overlap, but clearly assymetries going on with hpa as well
plot(char.res$hpa, ps.fcn(0,0,char.res$hpa,1))
ps.fcn <- function(v,c,pg,t){
#v_t <- (v-.25)/.5
v_t <- v
#ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))/5
#p<- (c*(acos(v_t))*atan(v_t^2)  - .5*exp(v_t))/4 + (t*((ihs_pg)) + (1-t))/2
ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))
p<- (1-t)*(c+1)*(acos(v_t)*atan(v_t) )/3 +
t*(.01+(-.01*ihs_pg^5 + 1*ihs_pg^3)/300)
p<- pmin(pmax(0,p),1)
return(p)
}
#story to accompany this fcn: ACLU wants to help those in trouble in "red states" but do not
#feel they can make a difference in really, really red states so target donors less often
plot(seq(0,1,.001),ps.fcn(seq(0,1,.001),4,800,0)) #a plot of the function
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),3,800,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),2,200,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),1,200,0))
plot(char.res$hpa, ps.fcn(0,0,char.res$hpa,1))
#char$mibush=char$perbush==-999
# Input from highly non-linear function
char.res$ps.true <- ps.fcn(char.res$perbush,char.res$cases,char.res$hpa,char.res$treatment) # hpa is highest previous contribution. cases is court cases from state which organization was involved.
# Plot CDF of this nonlinear function
ggplot(char.res,aes(x=ps.true))+ stat_ecdf()
# Set seed
set.seed(21)
# Selection rule (=1 of uniform random [0,1] is lower, so those with higher ps.true more likely to be selected)
selection <- runif(nrow(char.res)) <= char.res$ps.true
char.censored <- char.res[selection,] #remove observations via propensity score rule
ggplot(char.res,aes(x=perbush)) + geom_histogram()+xlim(c(0,1))
ggplot(char.censored,aes(x=ps.true)) + geom_histogram() +xlim(c(0,1))
#overlap in true propensity score
ggplot(char.censored,aes(x=ps.true,colour=factor(treatment))) + stat_ecdf()
ggplot(char.censored,aes(x=ps.true,y=hpa,colour=factor(treatment))) + geom_point()
#there is clear o
ggplot(char.censored,aes(x=ps.true,colour=factor(treatment))) + stat_ecdf()
covars.all <- char.censored[,c(14:22,23:63)] #skip the state indicator used for summ stats
tau.hat = residualBalance.ate(as.matrix(covars.all), char.censored$out_amountgive,
char.censored$treatment, estimate.se = TRUE,  optimizer = "pogs")
print(paste("true tau:", ate.true)) # 0.166066838677917
print(paste("point estimate:", round(tau.hat[1], 4))) #0.2058
print(paste0("95% CI for tau: (", round(tau.hat[1] - 1.96 * tau.hat[2], 2), ", ", round(tau.hat[1] + 1.96 * tau.hat[2], 2), ")"))
#setwd("C:/Users/Jack/Documents/Git/Athey ML homework 1/AtheyMLhw1") # Jack
setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
library(dplyr)
library(reshape2)
library(glmnet)
library(plotmo)
library(pogs)
library(balanceHD)
# set seed
set.seed(12345)
############################################################
# Load data
fname <- 'analysis/input/charitable_withdummyvariables.csv'
char <- read.csv(fname)
install.packages('rmarkdown')
install.packages("rmarkdown")
# set your working directory
#setwd("C:/Users/Jack/Documents/Git/Athey ML homework 1/AtheyMLhw1") # Jack
setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
library(dplyr)
library(reshape2)
library(glmnet)
library(plotmo)
library(pogs)
library(balanceHD)
# set seed
set.seed(12345)
############################################################
# Load data
fname <- 'analysis/input/charitable_withdummyvariables.csv'
char <- read.csv(fname)
attach(char) # attach so don't have to call each time
### Exploratory analysis
dim(char) # 50,083 obs, 63 vars
names(char)
head(char) # Look at first few entries of each var
# Treatment
summary(treatment) # Anyone who got any of the 27 treatments (3 match x 3 match size x 3 reccomended amount)
mean(treatment) # 67% treated
# Gives at all
summary(out_gavedum)
# Giving
summary(out_amountgive) # amount given. Highly skewed
hist(out_amountgive)
############################################################
### 1. Regression for average treatment effect
reg.ols <- lm(out_amountgive ~ treatment)
summary(reg.ols) # show results, significant at 90% but not 95% level
# Consistent with Table 4 of paper
confint(reg.ols, level=0.95) # CI
detach(char)
##############################
#randomly censor individuals
#via a  complex, highly nonlinear fcn  of votes 4 bush in state,
#
ps.fcn <- function(v,c,pg,t){
#v_t <- (v-.25)/.5
v_t <- v
#ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))/5
#p<- (c*(acos(v_t))*atan(v_t^2)  - .5*exp(v_t))/4 + (t*((ihs_pg)) + (1-t))/2
ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))
p<- (1-t)*(c+1)*(acos(v_t)*atan(v_t) )/3 +
t*(.01+(-.01*ihs_pg^5 + 1*ihs_pg^3)/300)
p<- pmin(pmax(0,p),1)
return(p)
}
#story to accompany this fcn: ACLU wants to help those in trouble in "red states" but do not
#feel they can make a difference in really, really red states so target donors less often
plot(seq(0,1,.001),ps.fcn(seq(0,1,.001),4,800,0)) #a plot of the function
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),3,800,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),2,200,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),1,200,0))
png(file='select_c.png')
plot(char$hpa, ps.fcn(0,0,char$hpa,1))
png(file='select_t.png')
plot(char$hpa, ps.fcn(0,0,char$hpa,1))
ps.fcn(0,0,char$hpa,1)
plot(char$hpa, ps.fcn(0,0,char$hpa,1))
png(file='select_t.png')
#feel they can make a difference in really, really red states so target donors less often
plot(seq(0,1,.001),ps.fcn(seq(0,1,.001),4,800,0)) #a plot of the function
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),3,800,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),2,200,0))
lines(seq(0,1,.001),ps.fcn(seq(0,1,.001),1,200,0))
jpeg(file='select_c')
